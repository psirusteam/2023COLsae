
# Día 2 - Sesión 1- Estimaciones GEIH y Función Generalizada de Varianza

```{r setup, include=FALSE, message=FALSE, error=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE)
library(printr)
library(kableExtra)
library(tidyverse)
tba <- function(dat, cap = NA){
  kable(dat,
      format = "html", digits =  4,
      caption = cap) %>% 
     kable_styling(bootstrap_options = "striped", full_width = F)%>%
         kable_classic(full_width = F, html_font = "Arial Narrow")
}

```

Uno de los insumos más importantes en el modelo de áreas es la 
varianza del estimador directo, a nivel de dominio, la cual no puede calcularse de ningún modo. En correspondencia, este valor debe estimarse desde los datos recolectados en cada dominio. Sin embargo, 
en dominios en las que se cuenta con un tamaño de muestra muy pequeño, estas estimaciones no tendrán un buen comportamiento. Por ende, es muy útil utilizar un modelo de **suavizamiento** de las varianzas para 
eliminar el ruido y la volatilidad de estas estimaciones y extraer la verdadera señal del proceso

Hidiroglou (2019) afirma que $E_{\mathscr{MP}}\left(\hat{\theta}^{dir}_d\right)=\boldsymbol{x}^{T}_{d}\boldsymbol{\beta}$ y $V_{\mathscr{MP}}\left(\hat{\theta}^{dir}_d\right)=\sigma_{u}^2+\tilde{\sigma}^2_{d}$, en donde el subíndice  $\mathscr{MP}$ hace referencia a la inferencia doble que se debe tener en cuenta en este tipo de ajustes y define la medida de probabilidad conjunta entre el modelo y el diseño de muestreo.  

-   $\mathscr{M}$ hace referencia a la medida de probabilidad inducida por el modelamiento y la inclusión de las covariables auxiliares ($\boldsymbol{x}_{d}$).

-   $\mathscr{MP}$ hace referencia a la medida de probabilidad inducida por el diseño de muestreo complejo que 
induce las estimaciones directas. 

La solución que acá se plantea se conoce con el nombre de Función Generalizada de Varianza, la cual consiste en ajustar un modelo log-lineal a la varianza directa estimada. Partiendo del hecho de que
se tiene acceso a un estimador insesgado de $\sigma^2$, denotado por $\hat{\sigma}^2$ se tiene que:
$$
E_{\mathscr{MP}}\left(\hat{\sigma}_{d}^{2}\right)=E_{\mathscr{M}}\left(E_{\mathscr{P}}\left(\hat{\sigma}_{d}^{2}\right)\right)=E_{\mathscr{M}}\left(\sigma_{d}^{2}\right)=\tilde{\sigma}_{d}^{2}
$$

La anterior igualdad puede interpretarse como que un estimador insesgado y simple de $\tilde{\sigma}_{d}^{2}$ puede ser $\hat{\sigma}_{d}^{2}$. Sin embargo, este estimador de muestreo es inestable cuando el tamaño de muestra es pequeño, que es justo el paradigma dominante en la estimación de áreas pequeñas. Rivest and Belmonte (2000) consideran modelos de suavizamiento para la estimación de las varianzas directas definidos de la siguiente manera:

$$
\log\left(\hat{\sigma}_{d}^{2}\right)=\boldsymbol{z}_{d}^{T}\boldsymbol{\alpha}+\boldsymbol{\varepsilon}_{d}
$$

En donde $\boldsymbol{z}_{d}$ es un vector de covariables explicativas que son funciones de $\boldsymbol{x}_{d}$, $\boldsymbol{\alpha}$ es un vector de parámetros que deben ser estimados, $\boldsymbol{\varepsilon}_{d}$ son errores aleatorios con media cero y varianza constante, que se asumen idénticamente distribuidos condicionalmente sobre $\boldsymbol{z}_{d}$. Del anterior modelo, la
estimación suavizada de la varianza de muestreo está dada por:
$$
\tilde{\sigma}_{d}^{2}=E_{\mathscr{MP}}\left(\sigma_{d}^{2}\right)=\exp\left(\boldsymbol{z}_{d}^{T}\boldsymbol{\alpha}\right)\times\Delta
$$

En donde, $E_{\mathscr{MP}}\left(\varepsilon_{d}\right)=\Delta$. No hay necesidad de especificar una distribución paramétrica para los
errores de este modelo. Al utilizar el método de los momentos, se tiene el siguiente estimador insesgado para $\Delta$: 
$$
\hat{\Delta}=\frac{\sum_{d=1}^{D}\hat{\sigma}_{d}^{2}}{\sum_{d=1}^{D}\exp\left(\boldsymbol{z}_{d}^{T}\boldsymbol{\alpha}\right)}
$$

De la misma forma, al utilizar los procedimientos estándar en una regresión lineal, la estimación del coeficiente de parámetros de regresión está dada por la siguiente expresión:

$$
\hat{\boldsymbol{\alpha}}=\left(\sum_{d=1}^{D}\boldsymbol{z}_{d}\boldsymbol{z}_{d}^{T}\right)^{-1}\sum_{d=1}^{D}\boldsymbol{z}_{d}\log\left(\hat{\sigma}_{d}^{2}\right)
$$

Por último, el estimador suavizado de la varianza muestral está definido por:

$$
\hat{\tilde{\sigma}}_{d}^{2}=\exp\left(\boldsymbol{z}_{d}^{T}\hat{\boldsymbol{\alpha}}\right)\hat{\Delta}
$$

## Datos de la encuesta

El siguiente bloque de código utiliza varias librerías en R (`tidyverse` y `magrittr`), así como también utiliza una función definida en otro archivo (source("0Recursos/0Source_FH.R")).

Luego, el código carga la encuesta que esta almacenada en un archivo de datos en formato RDS y utiliza la función `%>%` para encadenar una serie de transformaciones en los datos:

  -   `transmute()` se utiliza para seleccionar y renombrar columnas. En este caso, se seleccionan las columnas `dam_ee`, `dam2`, `_fep` y `segmento`, y se renombran a `dam`, `dam2`, `wkx`, y `upm`, respectivamente.
  
  -   Se crea una nueva variable llamada `estrato` que combina la variable `dam` con la variable `area_ee` después de convertir `area_ee` en una variable de factor utilizando `haven::as_factor()`. La función `paste0()` se utiliza para concatenar las dos variables.
  
  -   Se crea una nueva variable llamada `pobreza` que se establece en 1 si la variable `ingcorte`(ingreso percapital) es menor que la variable `lp`, y en 0 en caso contrario. La función `ifelse()` se utiliza para asignar valores a la variable "pobreza" en función de si el ingreso de un individuo es menor o mayor que el umbral de pobreza.

```{r}
library(tidyverse)
library(magrittr)
source("0Recursos/0Source_FH.R")

encuesta <- readRDS("Data/encuestaCOL18N1.rds") %>% 
  transmute(
    dam = dam_ee,
    dam2,
    wkx = `_fep`, 
    upm = segmento,
    estrato = paste0(dam, haven::as_factor(area_ee,levels = "values")),
    pobreza = ifelse(ingcorte < lp, 1 , 0))

```

-   *dam*: Corresponde al código asignado a la división administrativa mayor del país.

-   *dam2*: Corresponde al código asignado a la segunda división administrativa del país.

-   *lp* linea de pobreza definida por CEPAL. 

-   Factor de expansión por persona (*wkx*)


```{r, echo=FALSE}
tba(encuesta %>% head(10))
```

En el siguiente bloque de código  utiliza las librerías `survey` y `srvyr` para crear un diseño de muestreo a partir de una base de datos de encuestas. El diseño de muestreo incluye información sobre las unidades primarias de muestreo (UPM), los pesos de muestreo (wkx), y las estratas (estrato) utilizadas en el muestreo. Además, se utiliza la opción "survey.lonely.psu" para ajustar los tamaños de muestra en los grupos de unidades primarias de muestreo que no tienen otras unidades primarias de muestreo en el mismo grupo.

```{r}
library(survey)
library(srvyr)
options(survey.lonely.psu = "adjust")

diseno <-
  as_survey_design(
    ids = upm,
    weights = wkx,
    strata = estrato,
    nest = TRUE,
    .data = encuesta
  )
summary(diseno)
```

Para la estimación directa de la proporción se emplea la función `direct.supr`, disponible en el archivo `0Source_FH.R`. Está función realiza las estimaciones y criterios de calidad en una encuesta de muestreo complejo con diseño estratificado y por conglomerados. Toma cinco argumentos: `design.base`, `variable`, `group`, `upm` y `estrato`.

La función comienza cargando varios paquetes, como `rlang`, `tidyverse`, `dplyr`, `survey` y `srvyr.` Luego, los argumentos `group`, `variable`, `upm` y `estrato` se convierten en argumentos utilizando la función enquo.

La función utiliza la encuesta de muestreo complejo `design.base` para calcular las estimaciones de los parámetros y los criterios de calidad. Utiliza la función `survey_mean()` de la librería `survey` para calcular la media y los intervalos de confianza de la variable de interés. La función también calcula otros indicadores de calidad, como el coeficiente de variación, el tamaño de muestra efectivo y el efecto del diseño. Luego, utiliza la función `as.data.frame()` para convertir los resultados en un objeto de marco de datos.

Además, la función calcula otros criterios de calidad para determinar si las estimaciones son confiables. En particular, evalúa si se cumple un umbral mínimo para el número de grados de libertad, si la muestra es suficientemente grande y si el efecto del diseño es razonable. La función también tiene la opción de incluir o excluir ciertos grupos de muestreo basados en sus características.

```{r, eval=FALSE}
directodam2 <- direct.supr(design.base = diseno,
                             variable = pobreza, 
                             group = dam2,
                             upm = upm,
                             estrato = estrato)



directodam2 %>%
  group_by(Flag) %>%
  summarise(n = n()) %>% 
  arrange(n) %>% tba()
```

```{r, echo=FALSE}
directodam2 <- readRDS("Data/directodam2.rds")
directodam2 %>%
  group_by(Flag) %>%
  summarise(n = n()) %>% 
  arrange(n) %>% tba()
```


Para los dominios que no son excluidos se hace la transformación arcoseno, calculo del *DEFF* y varianza 

```{r}
base_sae <- directodam2 %>% 
  filter(Flag != "Excluir") %>%
  transmute(
    dam2 = dam2,             # Id para los dominios
    nd = n,                    # Número de observaciones por dominios
    n_effec = n.eff,           # n efectivo. 
    pobreza = p,               # Estimación de la variable
    pobreza_T = asin(sqrt(pobreza)), # Transformación arcoseno 
    vardir = ee ^ 2,                 # Estimación de la varianza directa 
    cv = CV,                       
    var_zd = 1 / (4 * n_effec),      # Varianza para la tranformación arcsin
    deff_dam2 = deff                # Deff por dominio
  )

# View(base_sae)
tba(head(base_sae))
```

seguidamente se realiza la transformación $\log(\hat{\sigma}^2_d)$, además se realiza la selección de las columnas identificador del municipio (`dam2`), la estimación directa  (`pobreza`), El número de personas en el dominio (`nd`) y la varianza estimada del para la estimación directa `vardir`,siendo esta la que transforma mediante la función `log()`. 

```{r}
baseFGV <-  base_sae %>% 
  select(dam2, pobreza, nd, vardir) %>%
  mutate(ln_sigma2 = log(vardir))

```

## Análisis gráfico

El primer gráfico, `p1`, muestra una gráfica de dispersión de la variable `ln_sigma2` en función de la variable `pobreza`, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como _pobreza_.

El segundo gráfico, `p2`, muestra una gráfica de dispersión de la variable `ln_sigma2` en función de la variable `nd`, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como _Tamaño de muestra_.

El tercer gráfico, `p3`, muestra una gráfica de dispersión de la variable `ln_sigma2` en función del producto de `pobreza` y `nd`, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como _Número de pobres_.

El cuarto gráfico, `p4`, muestra una gráfica de dispersión de la variable `ln_sigma2` en función de la raíz cuadrada de la variable `pobreza`, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como _Raiz cuadrada de pobreza_.


En general, los gráficos estan diseñados para explorar la relación entre `ln_sigma2` y diferentes variables independientes, como `pobreza`, `nd`, y la raíz cuadrada de la pobreza. La elección de utilizar la función "loess" para suavizar las líneas en lugar de una línea recta puede ayudar a visualizar mejor las tendencias generales en los datos.

```{r}
theme_set(theme_bw())

# pobreza vs Ln_sigma2 #

p1 <- ggplot(baseFGV, aes(x = pobreza, y = ln_sigma2)) +
  geom_point() +
  geom_smooth(method = "loess") +
  xlab("pobreza")

# Tamaño de muestra vs Ln_sigma2 #

p2 <- ggplot(baseFGV, aes(x = nd, y = ln_sigma2)) + 
  geom_point() +
  geom_smooth(method = "loess") + 
  xlab("Tamaño de muestra")

# Número de pobres vs Ln_sigma2 #

p3 <- ggplot(baseFGV, 
             aes(x = pobreza * nd, y = ln_sigma2)) + 
  geom_point() +
  geom_smooth(method = "loess") + 
  xlab("Número de pobres")

# Raiz_pobreza vs Ln_sigma2 #

p4 <- ggplot(baseFGV, 
             aes(x = sqrt(pobreza), y = ln_sigma2)) + 
  geom_point() +
  geom_smooth(method = "loess") + 
  xlab("Raiz cuadrada de pobreza")

library(patchwork)
(p1 | p2) / (p3 | p4)
```

## Modelo para la varianza

El código ajusta un modelo de regresión lineal múltiple (utilizando la función `lm()`), donde `ln_sigma2` es la variable respuesta y las variables predictoras son `pobreza`, `nd`, y varias transformaciones de éstas. El objetivo de este modelo es estimar la función generalizada de varianza (FGV) para los dominios observados.

```{r, results='asis'}
library(gtsummary)
FGV1 <- lm(ln_sigma2 ~ pobreza + I(nd^2) + I(sqrt(pobreza)),
     data = baseFGV)

tbl_regression(FGV1) %>% 
  add_glance_table(include = c(r.squared, adj.r.squared))
```

Después de tener la estimación del modelo se debe obtener el  valor de la constante $\Delta$ para lo cual se usa el siguiente código. 

```{r}
delta.hat = sum(baseFGV$vardir) / 
  sum(exp(fitted.values(FGV1)))

```

De donde se obtiene que $\Delta = `r delta.hat`$. Final es posible obtener la varianza suavizada  ejecutando el siguiente comando.  

```{r}
hat.sigma <- 
  data.frame(dam2 = baseFGV$dam2,
             hat_var = delta.hat * exp(fitted.values(FGV1)))

baseFGV <- left_join(baseFGV, hat.sigma)
tba(head(baseFGV, 10))
```

 Validación del modelo para la FGV

```{r}
par(mfrow = c(2, 2))
plot(FGV1)
```

 Comparación entre la varianza estimada versus la pronosticada por la FGV

```{r}
ggplot(baseFGV , 
       aes(y = vardir, x = hat_var)) + 
  geom_point() +
  geom_smooth(method = "loess") + 
    labs(x = "FGV", y = "VarDirEst") +
  ylab("Varianza del Estimador Directo")
```

Predicción de la varianza suavizada

```{r}
base_sae <- base_sae %>%  left_join(hat.sigma, by = "dam2")

```

El siguiente código utiliza la función `mutate()` del paquete `dplyr` para crear nuevas variables de la base de datos `base_sae` y luego guarda el resultado en un archivo RDS llamado `base_FH_2018.rds.`

En concreto, el código realiza las siguientes operaciones:

  -   La variable `deff_dam2` se ajusta a 1 cuando es NaN.
  
  -   La variable `deff_FGV` se calcula a partir de otras dos variables `hat_var` y `vardir.` Si `vardir` es 0, entonces `deff_FGV` se ajusta a 1. En caso contrario, se divide `hat_var` por v`ardir / deff_dam2` para obtener `deff_FGV.`

  -   La variable `deff_FGV` se regulariza utilizando el criterio MDS: si `deff_FGV` es menor que 1, se ajusta a 1.
  
  -   Finalmente, se calcula la variable `n_eff_FGV` dividiendo `nd` (el tamaño de la muestra) por `deff_FGV.`



```{r}
base_FH <- base_sae %>%
  mutate(
    deff_dam2 = ifelse(is.nan(deff_dam2), 1,
                         deff_dam2),
    deff_FGV = ifelse(
      vardir == 0 ,
      1,
      hat_var / (vardir / deff_dam2)
    ),
    # Criterio MDS para regularizar el DeffFGV
    deff_FGV = ifelse(deff_FGV < 1, 1, deff_FGV),
    n_eff_FGV = nd / deff_FGV
  )

saveRDS(object = base_FH, "Data/base_FH_2018.rds")
```


